#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import os, asyncio, re, sqlite3, random
from datetime import datetime
import feedparser
from telethon import TelegramClient
from telethon.sessions import MemorySession
from deep_translator import GoogleTranslator

# ========== ENVIRONMENT ==========
API_ID = int(os.getenv('API_ID', '39717958'))
API_HASH = os.getenv('API_HASH', 'e8e1f10ee0080cc64f3d8027a1de2088')
BOT_TOKEN = os.getenv('BOT_TOKEN', '')
KANAL = os.getenv('KANAL', '@xeberdunyasiaz')

# ========== DATABASE ==========
DB_FILE = 'news.db'

def init_db():
    """Database ba≈ülat"""
    conn = sqlite3.connect(DB_FILE)
    c = conn.cursor()
    c.execute('''CREATE TABLE IF NOT EXISTS posted_links
                 (link TEXT PRIMARY KEY, posted_at TIMESTAMP, type TEXT)''')
    conn.commit()
    conn.close()

def is_posted(link):
    """∆èvv…ôl payla≈üƒ±lƒ±bmƒ±?"""
    conn = sqlite3.connect(DB_FILE)
    c = conn.cursor()
    c.execute('SELECT link FROM posted_links WHERE link=?', (link,))
    result = c.fetchone()
    conn.close()
    return result is not None

def mark_posted(link, content_type='news'):
    """Payla≈üƒ±lƒ±b qeyd et"""
    conn = sqlite3.connect(DB_FILE)
    c = conn.cursor()
    try:
        c.execute('INSERT INTO posted_links VALUES (?, ?, ?)', 
                  (link, datetime.now(), content_type))
        conn.commit()
    except sqlite3.IntegrityError:
        pass
    conn.close()

# ========== X∆èB∆èR M∆èNB∆èL∆èR ==========
NEWS = {
    # Az…ôrbaycan (6)
    'APA': 'https://apa.az/rss/az/news',
    'Trend': 'https://az.trend.az/rss/',
    'Report': 'https://report.az/rss/',
    'Oxu.az': 'https://oxu.az/rss/news',
    'Azadliq': 'https://www.azadliq.org/api/zorrepgviq',
    'Milli.az': 'https://milli.az/rss',
    
    # T√ºrkiy…ô D√ºnya (8)
    'Anadolu': 'https://www.aa.com.tr/tr/rss/default?cat=dunya',
    'TRT D√ºnya': 'https://www.trthaber.com/dunya.rss',
    'H√ºrriyet': 'https://www.hurriyet.com.tr/rss/dunya',
    'NTV': 'https://www.ntv.com.tr/dunya.rss',
    'Sabah': 'https://www.sabah.com.tr/rss/dunya.xml',
    'S√∂zc√º': 'https://www.sozcu.com.tr/kategori/dunya/feed/',
    'Habert√ºrk': 'https://www.haberturk.com/rss/kategori/dunya.xml',
    'CNN T√ºrk': 'https://www.cnnturk.com/feed/rss/dunya/news',
    
    # Beyn…ôlxalq (10)
    'Reuters': 'http://feeds.reuters.com/reuters/topNews',
    'BBC': 'http://feeds.bbci.co.uk/news/world/rss.xml',
    'Al Jazeera': 'https://www.aljazeera.com/xml/rss/all.xml',
    'DW': 'https://rss.dw.com/rdf/rss-en-all',
    'CNN': 'http://rss.cnn.com/rss/edition_world.rss',
    'AP News': 'https://rsshub.app/apnews/topics/apf-topnews',
    'The Guardian': 'https://www.theguardian.com/world/rss',
    'France24': 'https://www.france24.com/en/rss',
    'Euro News': 'https://www.euronews.com/rss',
    'Sky News': 'https://feeds.skynews.com/feeds/rss/world.xml',
    
    # Elm & Tech (7)
    'ScienceDaily': 'https://www.sciencedaily.com/rss/top.xml',
    'BBC Science': 'http://feeds.bbci.co.uk/news/science_and_environment/rss.xml',
    'PopSci': 'https://www.popsci.com/feed/',
    'TechCrunch': 'https://techcrunch.com/feed/',
    'Wired': 'https://www.wired.com/feed/rss',
    'Ars Technica': 'https://feeds.arstechnica.com/arstechnica/index',
    'The Verge': 'https://www.theverge.com/rss/index.xml',
    
    # ƒ∞qtisad (4)
    'Bloomberg': 'https://www.bloomberg.com/feed/podcast/etf-iq.xml',
    'CNBC': 'https://www.cnbc.com/id/100003114/device/rss/rss.html',
    'MarketWatch': 'https://www.marketwatch.com/rss/topstories',
    'Financial Post': 'https://financialpost.com/feed/',
}

# ========== ∆èYL∆èNC∆è M∆èNB∆èL∆èR (Reddit) ==========
FUN = {
    'r/funny': 'https://www.reddit.com/r/funny/.rss',
    'r/memes': 'https://www.reddit.com/r/memes/.rss',
    'r/wholesomememes': 'https://www.reddit.com/r/wholesomememes/.rss',
    'r/Unexpected': 'https://www.reddit.com/r/Unexpected/.rss',
    'r/AnimalsBeingDerps': 'https://www.reddit.com/r/AnimalsBeingDerps/.rss',
    'r/aww': 'https://www.reddit.com/r/aww/.rss',
    'r/ContagiousLaughter': 'https://www.reddit.com/r/ContagiousLaughter/.rss',
}

AZ_SOURCES = ['APA', 'Trend', 'Report', 'Oxu.az', 'Azadliq', 'Milli.az']

# ========== FUNKSIYALAR ==========
def clean_html(text):
    """HTML t…ômizl…ô"""
    if not text: return ''
    text = re.sub(r'<[^>]+>', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def extract_numbers(text):
    """R…ôq…ôml…ôri vurƒüula"""
    if not text: return text
    text = re.sub(r'(\d+[\d\.,]*\s*%)', r'üìä \1', text)
    text = re.sub(r'([$‚Ç¨¬£¬•‚Ç∫]\s*[\d\.,]+[BMK]?)', r'üí∞ \1', text)
    return text

def generate_tags(title, content, source):
    """Avtomatik teql…ôr"""
    text = f"{title} {content}".lower()
    tags = set()
    
    countries = {
        'az…ôrbaycan': '#Az…ôrbaycan', 't√ºrkiy…ô': '#T√ºrkiy…ô', 'rusiya': '#Rusiya',
        'amerika': '#AB≈û', '√ßin': '#√áin', 'almaniya': '#Almaniya',
        'fransa': '#Fransa', 'ingilt…ôr…ô': '#ƒ∞ngilt…ôr…ô', 'iran': '#ƒ∞ran',
        'russia': '#Rusiya', 'turkey': '#T√ºrkiy…ô', 'usa': '#AB≈û', 'china': '#√áin',
    }
    
    topics = {
        'iqtisad': '#ƒ∞qtisadiyyat', 'siyas…ôt': '#Siyas…ôt', 'texnologiya': '#Texnologiya',
        'elm': '#Elm', 's…ôhiyy…ô': '#S…ôhiyy…ô', 'enerji': '#Enerji', 'neft': '#Neft',
        'economy': '#ƒ∞qtisadiyyat', 'politics': '#Siyas…ôt', 'technology': '#Texnologiya',
        'science': '#Elm', 'health': '#S…ôhiyy…ô', 'energy': '#Enerji', 'oil': '#Neft',
    }
    
    all_kw = {**countries, **topics}
    
    for word, tag in all_kw.items():
        if word in text:
            tags.add(tag)
    
    tags.add('#x…ôb…ôr')
    tags = list(tags)[:12]
    return ' '.join(sorted(tags))

def make_title(title):
    """Ba≈ülƒ±ƒüƒ± aktiv et"""
    if not title: return "üî¥ X∆èB∆èR"
    
    keywords = ['son d…ôqiq…ô', 't…ôcili', 'breaking', 'urgent']
    title_lower = title.lower()
    
    for kw in keywords:
        if kw in title_lower:
            return f"üî¥ SON D∆èQƒ∞Q∆è: {title}"
    
    return f"üî¥ SON D∆èQƒ∞Q∆è: {title}"

def get_news(url, source):
    """RSS-d…ôn x…ôb…ôr g√∂t√ºr"""
    try:
        feed = feedparser.parse(url)
        if not feed.entries:
            return None
        
        entry = feed.entries[0]
        title = clean_html(entry.get('title', ''))
        link = entry.get('link', '')
        
        if not title or not link:
            return None
        
        content = ''
        if 'summary' in entry:
            content = clean_html(entry.summary)
        elif 'description' in entry:
            content = clean_html(entry.description)
        
        if len(content) > 400:
            content = content[:397] + '...'
        
        media_url = None
        media_type = None
        
        if 'media_content' in entry:
            media = entry.media_content[0]
            media_url = media.get('url')
            media_type = 'video' if 'video' in media.get('type', '') else 'image'
        elif 'enclosures' in entry and entry.enclosures:
            enc = entry.enclosures[0]
            media_url = enc.get('href')
            media_type = 'video' if 'video' in enc.get('type', '') else 'image'
        elif 'media_thumbnail' in entry:
            media_url = entry.media_thumbnail[0].get('url')
            media_type = 'image'
        
        return {
            'title': title,
            'content': content,
            'link': link,
            'source': source,
            'media_url': media_url,
            'media_type': media_type
        }
    except Exception as e:
        print(f"‚ùå {source}: {str(e)}")
        return None

def extract_reddit_media(content_html):
    """Reddit post-dan media URL √ßƒ±xar (t…ôkmill…ô≈üdirilmi≈ü)"""
    if not content_html:
        return None, None
    
    # 1) Preview image (…ôn √ßox rast g…ôlin…ôn)
    match = re.search(r'https://preview\.redd\.it/[^\s"<>]+\.(?:jpg|png|gif|jpeg)', content_html)
    if match:
        return match.group(0), 'image'
    
    # 2) i.redd.it (direct image)
    match = re.search(r'https://i\.redd\.it/[^\s"<>]+\.(?:jpg|png|gif|jpeg)', content_html)
    if match:
        return match.group(0), 'image'
    
    # 3) v.redd.it (video) - Telethon bunu d…ôst…ôkl…ômir, amma link qalsƒ±n
    match = re.search(r'https://v\.redd\.it/[^\s"<>]+', content_html)
    if match:
        return match.group(0), 'video'
    
    # 4) External image (imgur.com v…ô s.)
    match = re.search(r'https://i\.imgur\.com/[^\s"<>]+\.(?:jpg|png|gif|jpeg)', content_html)
    if match:
        return match.group(0), 'image'
    
    # 5) Thumbnail (son √ßar…ô)
    match = re.search(r'<img[^>]+src="([^"]+)"', content_html)
    if match:
        url = match.group(1)
        # HTML entities decode
        url = url.replace('&amp;', '&')
        if url.startswith('http'):
            return url, 'image'
    
    return None, None

def get_fun_content(url, source):
    """Reddit-d…ôn …ôyl…ônc…ôli m…ôzmun g√∂t√ºr (t…ôkmill…ô≈üdirilmi≈ü)"""
    try:
        feed = feedparser.parse(url)
        if not feed.entries:
            return None
        
        # ƒ∞lk 20 post-dan media olan birini tap
        attempts = 0
        max_attempts = 20
        
        while attempts < max_attempts:
            # Random post se√ß
            entry = random.choice(feed.entries[:20])
            
            title = clean_html(entry.get('title', ''))
            link = entry.get('link', '')
            
            if not title or not link:
                attempts += 1
                continue
            
            # Artƒ±q g√∂nd…ôrilmi≈ümi?
            if is_posted(link):
                attempts += 1
                continue
            
            # Media tap
            content_html = entry.get('content', [{}])[0].get('value', '')
            media_url, media_type = extract_reddit_media(content_html)
            
            # Media varsa qaytar
            if media_url:
                return {
                    'title': title,
                    'link': link,
                    'source': source,
                    'media_url': media_url,
                    'media_type': media_type
                }
            
            attempts += 1
        
        # 20 c…ôhdd…ôn sonra he√ß n…ô tapƒ±lmadƒ±
        return None
        
    except Exception as e:
        print(f"‚ùå {source}: {str(e)}")
        return None

def tr(text):
    """Az…ôrbaycanca t…ôrc√ºm…ô"""
    try:
        return GoogleTranslator(source='auto', target='az').translate(text)
    except:
        return text

def generate_funny_caption(title):
    """G√ºlm…ôli a√ßƒ±qlama yarat"""
    captions = [
        f"üòÇ G√úLM∆èY∆è HAZIRSAN?\n\n{title}\n\nBunu g√∂r…ônd…ô g√ºlm…ôy…ô bilm…ôzs…ôn! ü§£",
        f"ü§£ BU N∆èDƒ∞R YA?\n\n{title}\n\nG√ºn√ºn …ôn g√ºlm…ôli anƒ±! üòÑ",
        f"üòÑ √áOX MARAQLI!\n\n{title}\n\nDostlarƒ±na g√∂nd…ôr, onlar da g√ºls√ºn! üéâ",
        f"üé≠ ∆èYL∆èNC∆èLƒ∞!\n\n{title}\n\nBir az g√ºlm…ôk he√ß k…ôs…ô z…ôr…ôr verm…ôz! üòä",
        f"üåü G√ñRM∆èLƒ∞S∆èN!\n\n{title}\n\nBu h…ôqiq…ôt…ôn g√ºlm…ôlidir! ü§™",
        f"üíØ ∆èFSAN∆è!\n\n{title}\n\nƒ∞nternetin …ôn g√ºlm…ôli m…ôzmunu! üòπ",
        f"üî• TOP!\n\n{title}\n\nBunu qa√ßƒ±rma, √ßox g√ºlm…ôlidir! üòÇ",
    ]
    return random.choice(captions)

async def post_news(client):
    """30 x…ôb…ôr payla≈ü"""
    posted = 0
    
    for source, url in NEWS.items():
        if posted >= 30:
            break
        
        try:
            news = get_news(url, source)
            
            if not news or is_posted(news['link']):
                continue
            
            # T…ôrc√ºm…ô
            if source not in AZ_SOURCES:
                news['title'] = tr(news['title'])
                if news['content']:
                    news['content'] = tr(news['content'])
            
            title = make_title(news['title'])
            content = extract_numbers(news['content']) if news['content'] else ''
            tags = generate_tags(news['title'], content, source)
            
            msg = f"{title}\n\n"
            if content:
                msg += f"{content}\n\n"
            msg += f"üì∞ {source} | Oxu: {news['link']}\n\n{tags}"
            
            try:
                if news['media_url']:
                    await client.send_file(KANAL, news['media_url'], caption=msg)
                else:
                    await client.send_message(KANAL, msg)
                
                mark_posted(news['link'], 'news')
                posted += 1
                print(f"‚úÖ [{posted}/30] {source}: G√∂nd…ôrildi")
                await asyncio.sleep(10)
                
            except Exception as e:
                print(f"‚ùå {source} g√∂nd…ôrm…ô x…ôta: {str(e)}")
        
        except Exception as e:
            print(f"‚ùå {source}: {str(e)}")
    
    return posted

async def post_fun(client):
    """1 …ôyl…ônc…ôli m…ôzmun payla≈ü (media olan)"""
    
    # B√ºt√ºn m…ônb…ôl…ôri qarƒ±≈üdƒ±r
    sources = list(FUN.items())
    random.shuffle(sources)
    
    for source, url in sources:
        try:
            fun = get_fun_content(url, source)
            
            # Media yoxdursa n√∂vb…ôti m…ônb…ôy…ô ke√ß
            if not fun or not fun['media_url']:
                print(f"‚ö†Ô∏è  {source}: Media tapƒ±lmadƒ±, skip")
                continue
            
            # T…ôrc√ºm…ô
            title_az = tr(fun['title'])
            
            # G√ºlm…ôli a√ßƒ±qlama
            caption = generate_funny_caption(title_az)
            caption += f"\n\nüì± M…ônb…ô: {source}\n\n#…ôyl…ônc…ô #g√ºlm…ôli #meme"
            
            try:
                # v.redd.it video i≈ül…ôm…ôy…ô bil…ôr, amma c…ôhd ed…ôk
                await client.send_file(KANAL, fun['media_url'], caption=caption)
                
                mark_posted(fun['link'], 'fun')
                print(f"üòÇ ∆èyl…ônc…ô g√∂nd…ôrildi: {source}")
                return True
                
            except Exception as e:
                print(f"‚ùå {source} g√∂nd…ôrm…ô x…ôta: {str(e)}")
                # Media g√∂nd…ôrilm…ôs…ô n√∂vb…ôti m…ônb…ôy…ô ke√ß
                continue
        
        except Exception as e:
            print(f"‚ùå {source}: {str(e)}")
    
    print("‚ö†Ô∏è  He√ß bir …ôyl…ônc…ôli m…ôzmun g√∂nd…ôril…ô bilm…ôdi")
    return False

async def main():
    """∆èsas funksiya"""
    print("\n" + "="*60)
    print("ü§ñ X∆èB∆èR D√úNYASI BOT - VERSƒ∞YA 4.1 ULTRA")
    print("="*60)
    print("‚úÖ X…ôb…ôr + ∆èyl…ônc…ô hibrid kanal!")
    print(f"üì¢ Kanal: {KANAL}")
    print(f"üì∞ X…ôb…ôr m…ônb…ô: {len(NEWS)} keyfiyy…ôtli")
    print(f"üòÇ ∆èyl…ônc…ô m…ônb…ô: {len(FUN)} subreddit")
    print("üîÑ Format: 30 x…ôb…ôr ‚Üí 1 …ôyl…ônc…ô ‚Üí 6 d…ôqiq…ô")
    print("üéØ ∆èyl…ônc…ô: Yalnƒ±z media olan (≈ü…ôkil/video)")
    print("üîí T…ôkrar yoxlama: aktiv")
    print("="*60 + "\n")
    
    init_db()
    
    async with TelegramClient(MemorySession(), API_ID, API_HASH) as c:
        await c.start(bot_token=BOT_TOKEN)
        
        while True:
            print(f"\n{'='*60}")
            print(f"üîÑ YENƒ∞ D√ñVR: {datetime.now().strftime('%H:%M:%S')}")
            print(f"{'='*60}\n")
            
            # 30 x…ôb…ôr
            posted = await post_news(c)
            print(f"\nüìä {posted} x…ôb…ôr g√∂nd…ôrildi\n")
            
            # 1 …ôyl…ônc…ô (media olan)
            await post_fun(c)
            
            print(f"\n{'='*60}")
            print("‚è∏ 6 d…ôqiq…ô fasil…ô...")
            print(f"{'='*60}\n")
            
            await asyncio.sleep(360)

if __name__ == '__main__':
    asyncio.run(main())
