import os
import re
import asyncio
import feedparser
from datetime import datetime
from telethon import TelegramClient
from deep_translator import GoogleTranslator

# Environment variables
API_ID = int(os.getenv('API_ID', '39717958'))
API_HASH = os.getenv('API_HASH', 'e8e1f10ee0080cc64f3d8027a1de2088')
BOT_TOKEN = os.getenv('BOT_TOKEN', 'BURAYA_YENƒ∞_BOT_TOKEN')
KANAL = os.getenv('KANAL', '@xeberdunyasiaz')

# 18 keyfiyy…ôtli m…ônb…ô
NEWS = {
    'APA': 'https://apa.az/rss/az/news',
    'Trend': 'https://az.trend.az/rss/',
    'Report': 'https://report.az/rss/',
    'Oxu.az': 'https://oxu.az/rss/news',
    'Anadolu': 'https://www.aa.com.tr/tr/rss/default?cat=dunya',
    'TRT D√ºnya': 'https://www.trthaber.com/dunya.rss',
    'H√ºrriyet': 'https://www.hurriyet.com.tr/rss/dunya',
    'NTV': 'https://www.ntv.com.tr/dunya.rss',
    'Reuters': 'http://feeds.reuters.com/reuters/topNews',
    'BBC': 'http://feeds.bbci.co.uk/news/world/rss.xml',
    'Al Jazeera': 'https://www.aljazeera.com/xml/rss/all.xml',
    'DW': 'https://rss.dw.com/rdf/rss-en-all',
    'CNN': 'http://rss.cnn.com/rss/edition_world.rss',
    'Evrim Aƒüacƒ±': 'https://evrimagaci.org/rss',
    'Nat Geo': 'https://www.nationalgeographic.com/pages/topic/latest-stories/_jcr_content.feed',
    'ScienceDaily': 'https://www.sciencedaily.com/rss/top.xml',
    'BBC Science': 'http://feeds.bbci.co.uk/news/science_and_environment/rss.xml',
    'PopSci': 'https://www.popsci.com/feed/'
}

AZ_SOURCES = ['APA', 'Trend', 'Report', 'Oxu.az']

# Teq s√∂zl…ôri
TAG_KEYWORDS = {
    '√∂lk…ôl…ôr': ['AB≈û', 'Rusiya', '√áin', 'T√ºrkiy…ô', 'ƒ∞ran', 'Az…ôrbaycan', 'Ukrayna', 'Almaniya', 'Fransa', 'Britaniya'],
    '≈ü…ôxsl…ôr': ['Putin', 'Biden', 'Erdoƒüan', 'Zelenski', 'Trump', 'Prezident', 'Nazir'],
    'm√∂vzular': ['ƒ∞qtisadiyyat', 'Siyas…ôt', 'Elm', 'Texnologiya', 'ƒ∞dman', 'Maliyy…ô', 'Enerji', 'Neft', 'Qaz'],
    'hadis…ôl…ôr': ['Sanksiya', 'M√ºharib…ô', 'Sammit', 'G√∂r√º≈ü', 'Danƒ±≈üƒ±q', 'Razƒ±la≈üma', 'Q…ôrar']
}

def clean_html(text):
    text = re.sub('<[^<]+?>', '', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def extract_numbers(text):
    text = re.sub(r'(\$\d+[\d\.,]*\s*(milyard|million|bn|mn)?)', r'üí∞ \1', text)
    text = re.sub(r'(\d+[\d\.,]*%)', r'üìä \1', text)
    text = re.sub(r'(\d+\+)', r'üìà \1', text)
    return text

def generate_tags(title, content):
    tags = ['x…ôb…ôr']
    combined = (title + ' ' + content).lower()
    
    for country in TAG_KEYWORDS['√∂lk…ôl…ôr']:
        if country.lower() in combined:
            tags.append(country)
    
    for person in TAG_KEYWORDS['≈ü…ôxsl…ôr']:
        if person.lower() in combined:
            tags.append(person)
    
    for topic in TAG_KEYWORDS['m√∂vzular']:
        if topic.lower() in combined:
            tags.append(topic)
    
    for event in TAG_KEYWORDS['hadis…ôl…ôr']:
        if event.lower() in combined:
            tags.append(event)
    
    tags = list(set(tags))[:12]
    return ' '.join([f'#{t.replace(" ", "")}' for t in tags])

def make_title(title):
    t = title.lower()
    if any(w in t for w in ['breaking', 'urgent', 't…ôcili', 'son d…ôqiq…ô']):
        return f"üî¥ SON D∆èQƒ∞Q∆è: {title}"
    elif any(w in t for w in ['shock', '≈üok', 'sensasiya', 'ilk d…ôf…ô']):
        return f"‚ö° ≈ûOK: {title}"
    elif any(w in t for w in ['prezident', 'president', 'lider']):
        return f"üèõ T∆èCƒ∞Lƒ∞: {title}"
    elif any(w in t for w in ['rekord', 'record', 'tarixi']):
        return f"üìà REKORD: {title}"
    else:
        return f"üì∞ {title}"

def add_timestamp():
    now = datetime.now()
    return f"‚è± {now.strftime('%H:%M')} | {now.strftime('%d %B')}"

def get_news(url):
    try:
        f = feedparser.parse(url)
        if f.entries:
            e = f.entries[0]
            
            title = e.title
            content = ''
            if hasattr(e, 'summary'):
                content = clean_html(e.summary)[:400]
            elif hasattr(e, 'description'):
                content = clean_html(e.description)[:400]
            
            media_url = None
            media_type = None
            
            if hasattr(e, 'enclosures') and e.enclosures:
                enc = e.enclosures[0]
                if hasattr(enc, 'type'):
                    if 'video' in enc.type:
                        media_url = enc.href
                        media_type = 'video'
                    elif 'image' in enc.type:
                        media_url = enc.href
                        media_type = 'image'
            
            if not media_url and hasattr(e, 'media_content'):
                for m in e.media_content:
                    if 'url' in m:
                        media_url = m['url']
                        media_type = 'image'
                        break
            
            if not media_url and hasattr(e, 'media_thumbnail'):
                if e.media_thumbnail and len(e.media_thumbnail) > 0:
                    media_url = e.media_thumbnail[0].get('url')
                    media_type = 'image'
            
            return {
                'title': title,
                'content': content,
                'link': e.link,
                'source': f.feed.title if hasattr(f.feed, 'title') else 'M…ônb…ô',
                'media_url': media_url,
                'media_type': media_type
            }
    except:
        pass
    return None

def tr(text):
    try:
        return GoogleTranslator(source='auto', target='az').translate(text)
    except:
        return text

async def post(c):
    print(f"\nüîÑ [{datetime.now().strftime('%H:%M')}] X…ôb…ôrl…ôr toplanƒ±r...")
    
    all_news = []
    for i, (name, url) in enumerate(NEWS.items(), 1):
        print(f"   [{i}/18] {name}...", end=' ')
        x = get_news(url)
        if x:
            need_tr = name not in AZ_SOURCES
            title = tr(x['title']) if need_tr else x['title']
            content = tr(x['content']) if need_tr and x['content'] else x['content']
            
            content = extract_numbers(content)
            title = make_title(title)
            tags = generate_tags(title, content)
            
            all_news.append({
                'name': name,
                'title': title,
                'content': content,
                'link': x['link'],
                'media_url': x['media_url'],
                'media_type': x['media_type'],
                'tags': tags
            })
            print(f"‚úÖ")
        else:
            print(f"‚ùå")
    
    print(f"\nüì¢ Kanala payla≈üƒ±lƒ±r...")
    for i, news in enumerate(all_news):
        try:
            text = f"{news['title']}\n\n"
            text += f"{add_timestamp()}\n\n"
            
            if news['content']:
                text += f"{news['content']}\n\n"
            
            text += f"üì∞ {news['name']} | Oxu: {news['link']}\n\n"
            text += news['tags']
            
            if news['media_url'] and news['media_type'] == 'video':
                await c.send_file(KANAL, news['media_url'], caption=text)
            elif news['media_url'] and news['media_type'] == 'image':
                await c.send_file(KANAL, news['media_url'], caption=text)
            else:
                await c.send_message(KANAL, text)
            
            print(f"   ‚úÖ [{i+1}/{len(all_news)}] {news['name']}")
            
            if (i + 1) % 3 == 0 and i + 1 < len(all_news):
                await asyncio.sleep(150)
            else:
                await asyncio.sleep(10)
        except Exception as e:
            print(f"   ‚ùå [{i+1}/{len(all_news)}] X…ôta: {e}")
    
    print(f"‚úÖ D√∂vr√º tamamlandƒ±: {len(all_news)} x…ôb…ôr\n")

async def main():
    print("=" * 50)
    print("ü§ñ X∆èB∆èR D√úNYASI BOT - VERSƒ∞YA 4.0 PROFESSIONAL")
    print("=" * 50)
    print(f"‚úÖ Bot i≈ü…ô d√º≈üd√º!")
    print(f"üì¢ Kanal: {KANAL}")
    print(f"üåç M…ônb…ô: 18 keyfiyy…ôtli")
    print(f"‚è∞ Yenil…ônm…ô: H…ôr 6 d…ôqiq…ô")
    print(f"üì∏ Media: RSS-d…ôn (varsa)")
    print(f"üè∑ Teql…ôr: 8-12 avtomatik")
    print(f"üìä R…ôq…ôml…ôr: avtomatik vurƒüu")
    print("=" * 50 + "\n")
    
    c = TelegramClient('bot', API_ID, API_HASH)
    await c.start(bot_token=BOT_TOKEN)
    
    while True:
        await post(c)
        print(f"‚è∞ 6 d…ôqiq…ô g√∂zl…ôyir...")
        await asyncio.sleep(360)

if __name__ == '__main__':
    asyncio.run(main())
